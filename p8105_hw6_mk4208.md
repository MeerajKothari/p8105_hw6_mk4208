p8105\_hw6\_mk4208
================
Meeraj Kothari
11/19/2019

# Loading libraries

``` r
library(tidyverse)
library(modelr)
library(mlbench)
library(caret)
library(patchwork)
```

# Problem 1

Following code chunk loads and cleaning the data for regression analysis
and checks for missing data producing a
summary.

``` r
birthweight = read_csv("./Data/birthweight.csv") %>% janitor::clean_names() %>% 
  mutate(babysex = factor(babysex, c(1, 2), c("Male", "Female")),
         frace = factor(frace, c(1:4, 8, 9), c("White", "Black", "Asian", "Puerto Rican", "Other", "Unkown")),
         malform = factor(malform, c(0, 1), c("Absent", "Present")),
         mrace = factor(mrace, c(1:4, 8), c("White", "Black", "Asian", "Puerto Rican", "Other")))

skimr::skim(birthweight)
```

    ## Skim summary statistics
    ##  n obs: 4342 
    ##  n variables: 20 
    ## 
    ## ── Variable type:factor ────────────────────────────────────────────────────────────────────────
    ##  variable missing complete    n n_unique
    ##   babysex       0     4342 4342        2
    ##     frace       0     4342 4342        5
    ##   malform       0     4342 4342        2
    ##     mrace       0     4342 4342        4
    ##                               top_counts ordered
    ##              Mal: 2230, Fem: 2112, NA: 0   FALSE
    ##  Whi: 2123, Bla: 1911, Pue: 248, Asi: 46   FALSE
    ##                Abs: 4327, Pre: 15, NA: 0   FALSE
    ##  Whi: 2147, Bla: 1909, Pue: 243, Asi: 43   FALSE
    ## 
    ## ── Variable type:numeric ───────────────────────────────────────────────────────────────────────
    ##  variable missing complete    n      mean     sd     p0     p25     p50
    ##     bhead       0     4342 4342   33.65     1.62  21      33      34   
    ##   blength       0     4342 4342   49.75     2.72  20      48      50   
    ##       bwt       0     4342 4342 3114.4    512.15 595    2807    3132.5 
    ##     delwt       0     4342 4342  145.57    22.21  86     131     143   
    ##   fincome       0     4342 4342   44.11    25.98   0      25      35   
    ##   gaweeks       0     4342 4342   39.43     3.15  17.7    38.3    39.9 
    ##  menarche       0     4342 4342   12.51     1.48   0      12      12   
    ##   mheight       0     4342 4342   63.49     2.66  48      62      63   
    ##    momage       0     4342 4342   20.3      3.88  12      18      20   
    ##    parity       0     4342 4342    0.0023   0.1    0       0       0   
    ##   pnumlbw       0     4342 4342    0        0      0       0       0   
    ##   pnumsga       0     4342 4342    0        0      0       0       0   
    ##     ppbmi       0     4342 4342   21.57     3.18  13.07   19.53   21.03
    ##      ppwt       0     4342 4342  123.49    20.16  70     110     120   
    ##    smoken       0     4342 4342    4.15     7.41   0       0       0   
    ##    wtgain       0     4342 4342   22.08    10.94 -46      15      22   
    ##      p75   p100     hist
    ##    35      41   ▁▁▁▁▅▇▁▁
    ##    51      63   ▁▁▁▁▁▇▁▁
    ##  3459    4791   ▁▁▁▃▇▇▂▁
    ##   157     334   ▁▇▅▁▁▁▁▁
    ##    65      96   ▁▂▇▂▂▂▁▃
    ##    41.1    51.3 ▁▁▁▁▃▇▁▁
    ##    13      19   ▁▁▁▁▂▇▁▁
    ##    65      77   ▁▁▁▅▇▂▁▁
    ##    22      44   ▂▇▅▂▁▁▁▁
    ##     0       6   ▇▁▁▁▁▁▁▁
    ##     0       0   ▁▁▁▇▁▁▁▁
    ##     0       0   ▁▁▁▇▁▁▁▁
    ##    22.91   46.1 ▁▇▅▁▁▁▁▁
    ##   134     287   ▁▇▆▁▁▁▁▁
    ##     5      60   ▇▁▁▁▁▁▁▁
    ##    28      89   ▁▁▁▇▇▁▁▁

The following code chunks use the ‘caret’ package and the ‘mlbench’
package to rank feature importance.

  - First, I produce a correlation matrix of the all the numeric
    variables and set the cut off at 0.7 in order to remove highly
    correlated.

<!-- end list -->

``` r
set.seed(10)

nums = map(birthweight, is.numeric) %>% unlist()

correlationMatrix = birthweight %>% select_if(is.numeric) %>% cor() 

highlyCorrelated = correlationMatrix %>% findCorrelation(cutoff = 0.7)

birthweight_no_corr = birthweight %>% select(-highlyCorrelated, "bwt")
```

  - Second, I do 10 repeat of 10-fold CV for the data and train a
    generalized linear model for the ‘bwt’ variable. The ‘varImp’
    function ranks the importance based on the absolute value of the
    t-statistic for each model parameter used.

<!-- end list -->

``` r
control = trainControl(method = "cv", number = 10, repeats = 10)

model = train(bwt~., data = birthweight_no_corr, method = "glm", preProcess = "scale", trControl = control)

importance = varImp(model, scale = TRUE)
```

  - Third, I plot the imporance of each parameter.

<!-- end list -->

``` r
ggplot(importance) + 
  theme_minimal()
```

![](p8105_hw6_mk4208_files/figure-gfm/unnamed-chunk-5-1.png)<!-- -->

Based on the above produced plot, I select ‘bhead’, ‘delwt’, ‘gaweeks’,
‘smoken’, ‘mrace’, ‘babysex’ as my
predictors.

``` r
fit = lm(bwt ~ bhead + delwt + gaweeks + smoken + mrace + babysex, data = birthweight)
```

``` r
birthweight %>% 
  modelr::add_residuals(fit) %>% 
  modelr::add_predictions(fit) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point() + 
  theme_minimal()
```

![](p8105_hw6_mk4208_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->

``` r
comparison_1 = lm(bwt ~ blength + gaweeks, data = birthweight)

comparison_2 = lm(bwt ~ bhead + blength + babysex + bhead*blength + blength*babysex + babysex*bhead + bhead*blength*babysex, data = birthweight) 

cv_df = 
  crossv_mc(birthweight, 100) %>% 
  mutate(
    train = map(train, as_tibble), 
    test = map(test, as_tibble))

cv_df = cv_df %>%
  mutate(fit = map(train, ~lm(bwt ~ bhead + delwt + gaweeks + smoken + mrace + babysex, data = .x)),
         comparison_1 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         comparison_2 = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + blength*babysex + babysex*bhead + bhead*blength*babysex, data = .x))) %>% 
  mutate(rmse_fit = map2_dbl(fit, test, ~rmse(model = .x, data = .y)),
         rmse_comparison_1 = map2_dbl(comparison_1, test, ~rmse(model = .x, data = .y)),
         rmse_comparison_2 = map2_dbl(comparison_2, test, ~rmse(model = .x, data = .y)))
```

``` r
cv_df %>% 
  select(starts_with("rmse")) %>%
  pivot_longer(
    everything(), 
    names_to = "model",
    values_to = "rmse", 
    names_prefix = "rmse_") %>%
  mutate(model = fct_inorder(model)) %>%
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin()
```

![](p8105_hw6_mk4208_files/figure-gfm/unnamed-chunk-9-1.png)<!-- -->

# Problem 2

``` r
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

``` r
results = weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~ lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::tidy),
    glance = map(models, broom::glance)) %>% 
  select(results, glance, .id) %>% 
  unnest(results) %>% 
  pivot_wider( 
    names_from = term,
    values_from = c(estimate, std.error, glance),
    id_cols = .id, 
    ) %>%
  janitor::clean_names() %>%
  unnest(c(glance_intercept), .names_repair = "universal") %>% 
  mutate(
    log_b0_b1 = log(estimate_intercept * estimate_tmin)
  ) 
```

    ## Warning: unnest() has a new interface. See ?unnest for details.
    ## Try `df %>% unnest(c(`c(glance_intercept)`, .names_repair))`, with `mutate()` if needed

``` r
p1 = results %>% 
  ggplot(aes(x = r.squared)) + 
  geom_histogram() + 
  geom_density() +
  theme_minimal() 

p2 = results %>% 
  ggplot(aes(x = log_b0_b1)) + 
  geom_histogram() + 
  geom_density() +
  theme_minimal() 

p1 + p2
```

    ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
    ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

![](p8105_hw6_mk4208_files/figure-gfm/unnamed-chunk-12-1.png)<!-- -->
