---
title: "p8105_hw6_mk4208"
author: "Meeraj Kothari"
date: "11/19/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Loading libraries

```{r message=FALSE}
library(tidyverse)
library(modelr)
library(mlbench)
library(caret)
library(patchwork)
library(viridis)
```


# Problem 1 

Following code chunk loads and cleaning the data for regression analysis and checks for missing data producing a summary.

```{r message = FALSE}
birthweight = read_csv("./Data/birthweight.csv") %>% janitor::clean_names() %>% 
  mutate(babysex = factor(babysex, c(1, 2), c("Male", "Female")),
         frace = factor(frace, c(1:4, 8, 9), c("White", "Black", "Asian", "Puerto Rican", "Other", "Unkown")),
         malform = factor(malform, c(0, 1), c("Absent", "Present")),
         mrace = factor(mrace, c(1:4, 8), c("White", "Black", "Asian", "Puerto Rican", "Other")))

skimr::skim(birthweight)
```

The following code chunks use the `caret` package and the `mlbench` package to rank feature importance. 

* First, I do 10 repeats of 10-fold CV for the data and train a generalized linear model for the `bwt` variable. The `varImp` function ranks the importance based on the absolute value of the t-statistic for each model parameter used. 

```{r warning = FALSE}
set.seed(10)

control = trainControl(method = "cv", number = 10, repeats = 10)

model = train(bwt~., data = birthweight, method = "glm", preProcess = "scale", trControl = control)

importance = varImp(model, scale = TRUE)
```

* Second, I plot the imporance of each parameter.

```{r}
ggplot(importance) + 
  theme_minimal()
```

Based on the above produced plot, I select `bhead`, `blength`, `delwt`, `smoken`, `babysex`, `gaweeks` and `mrace` as my predictors and fit a linear model.  

```{r}
my_model = lm(bwt ~ bhead + blength + delwt + smoken + gaweeks + babysex + mrace, data = birthweight)
```

The following code chunk produces a plot of model residuals against fitted values using `add_predictions` and `add_residuals`. 

```{r}
birthweight %>% 
  modelr::add_residuals(my_model) %>% 
  modelr::add_predictions(my_model) %>% 
  ggplot(aes(x = pred, y = resid, color = resid)) +
  geom_point(alpha = 0.5) + 
  scale_color_viridis() +
  theme_minimal() +
  theme(legend.position = "none")
```

Following code chunks produces two comparision models: 

* `comparison_1` uses length at birth and gestational age as predictors (main effects only)

* `comparison_2` uses head circumference, length, sex, and all interactions (including the three-way interaction) between these. 

```{r}
comparison_1 = lm(bwt ~ blength + gaweeks, data = birthweight)

comparison_2 = lm(bwt ~ bhead + blength + babysex + bhead*blength + blength*babysex + babysex*bhead + bhead*blength*babysex, data = birthweight) 
```

Following code chunk compares the 3 models in terms of the cross-validated prediction error, using `crossv_mc`. 

```{r}
cv_df = 
  crossv_mc(birthweight, 100) %>% 
  mutate(
    train = map(train, as_tibble), 
    test = map(test, as_tibble))

cv_df = cv_df %>%
  mutate(my_model = map(train, ~lm(bwt ~ bhead + blength + delwt + smoken + gaweeks + babysex + mrace, data = .x)),
         comparison_1 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         comparison_2 = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + blength*babysex + babysex*bhead + bhead*blength*babysex, data = .x))) %>% 
  mutate(rmse_my_model = map2_dbl(my_model, test, ~rmse(model = .x, data = .y)),
         rmse_comparison_1 = map2_dbl(comparison_1, test, ~rmse(model = .x, data = .y)),
         rmse_comparison_2 = map2_dbl(comparison_2, test, ~rmse(model = .x, data = .y)))

cv_df %>% 
  select(starts_with("rmse")) %>%
  pivot_longer(
    everything(), 
    names_to = "model",
    values_to = "rmse", 
    names_prefix = "rmse_") %>%
  mutate(model = fct_inorder(model)) %>%
  ggplot(aes(x = model, y = rmse, fill = model)) + 
  geom_violin() + 
  scale_fill_viridis_d(alpha = 0.5) + 
  theme_minimal() + 
  theme(legend.position = "none")
```

# Problem 2

For this problem, we’ll use the 2017 Central Park weather data that we’ve seen elsewhere. The code chunk below (adapted from the course website) will download these data.

```{r message = FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

The following code chunk uses 5000 bootstrap samples and, for each bootstrap sample, produce estimates of r² and log(β0*β1).

```{r message=FALSE}
results = weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~ lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::tidy),
    glance = map(models, broom::glance)) %>% 
  select(results, glance, .id) %>% 
  unnest(results) %>% 
  pivot_wider( 
    names_from = term,
    values_from = c(estimate, std.error, glance),
    id_cols = .id, 
    ) %>%
  janitor::clean_names() %>%
  unnest(c(glance_intercept), .names_repair = "universal") %>% 
  mutate(
    log_b0_b1 = log(estimate_intercept * estimate_tmin)
  ) 
```

The following code chunk plots the distribution of r² and log(β0 * β1)  

```{r message=FALSE}
p1 = results %>% 
  ggplot(aes(x = r.squared)) + 
  geom_histogram() + 
  geom_density() +
  theme_minimal() 

p2 = results %>% 
  ggplot(aes(x = log_b0_b1)) + 
  geom_histogram() + 
  geom_density() +
  theme_minimal() 

p1 + p2
```

Both the distributions appear normally distributed. The mean of r² is `r results %>% pull(r.squared) %>% mean` and the median of r² is `r results %>% pull(r.squared) %>% median`.
The mean of log(β0 * β1) is `r results %>% pull(log_b0_b1) %>% mean` and the median of log(β0 * β1) is `r results %>% pull(log_b0_b1) %>% median`. 

-----------

The following tables give the 2.5% and 97.5% quantiles for the r² and log(β0 * β1) for the 95% confidence interval. 

```{r}
results %>% 
  pull(r.squared) %>% 
  quantile(c(0.025, 0.975)) %>% 
  knitr::kable(col.names = "R-Squared")

results %>% 
  pull(log_b0_b1) %>% 
  quantile(c(0.025, 0.975)) %>% 
  knitr::kable(col.names = "Log(B0*B1)")
```



